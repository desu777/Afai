"""
Prompt Saver Module
Saves LLM prompts to files for analysis when SAVE_PROMPT=true
"""
import os
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
from models import ConversationState

class PromptSaver:
    """
    Handles saving prompts to organized directory structure per session
    """
    
    def __init__(self, session_id: str = "unknown"):
        self.session_id = session_id
        self.base_path = Path(__file__).parent.parent / "check_prompts" / f"session_{session_id}"
        self.counter_file = self.base_path / ".counter"
        self.workflow_data = []
        
        # Ensure directory exists
        self.base_path.mkdir(parents=True, exist_ok=True)
        
    def _get_next_counter(self) -> int:
        """Get next counter number for this session"""
        if self.counter_file.exists():
            try:
                with open(self.counter_file, 'r') as f:
                    counter = int(f.read().strip()) + 1
            except (ValueError, FileNotFoundError):
                counter = 1
        else:
            counter = 1
            
        # Save updated counter
        with open(self.counter_file, 'w') as f:
            f.write(str(counter))
            
        return counter
    
    def save_prompt(self, node_name: str, prompt_content: str, metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Save prompt to markdown file with metadata
        
        Args:
            node_name: Name of the workflow node (e.g., "intent_detector")
            prompt_content: The actual prompt content sent to LLM
            metadata: Additional metadata (user_query, intent, language, etc.)
        """
        if not metadata:
            metadata = {}
            
        counter = self._get_next_counter()
        filename = f"{counter:03d}_{node_name}.md"
        filepath = self.base_path / filename
        
        # Format timestamp
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Create markdown content
        llm_response_time = metadata.get('llm_response_time')
        response_time_line = f"**LLM Response Time:** {llm_response_time:.2f}ms  \n" if llm_response_time is not None else ""
        
        md_content = f"""# {node_name.replace('_', ' ').title()} Prompt

{response_time_line}**Session:** {self.session_id}  
**Timestamp:** {timestamp}  
**User Query:** {metadata.get('user_query', 'N/A')}  
**Intent:** {metadata.get('intent', 'N/A')}  
**Language:** {metadata.get('detected_language', 'N/A')}  

## Prompt Content

```
{prompt_content}
```

## Response Metadata

- Model: {metadata.get('model_name', 'N/A')}
- Temperature: {metadata.get('temperature', 'N/A')}
- Response length: {metadata.get('response_length', 'N/A')} chars
- Node execution time: {metadata.get('execution_time', 'N/A')}s

---
*Generated by Aquaforest RAG Prompt Inspector*
"""
        
        # Write to file
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
            
        # Store for workflow summary
        self.workflow_data.append({
            "counter": counter,
            "node_name": node_name,
            "filename": filename,
            "timestamp": timestamp,
            "prompt_length": len(prompt_content),
            "metadata": metadata
        })
    
    def save_workflow_summary(self, total_execution_time: float = None) -> None:
        """
        Save workflow summary as JSON
        
        Args:
            total_execution_time: Total workflow execution time in seconds
        """
        if not self.workflow_data:
            return
            
        summary = {
            "session_id": self.session_id,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "user_query": self.workflow_data[0]["metadata"].get("user_query", "N/A"),
            "detected_language": self.workflow_data[0]["metadata"].get("detected_language", "N/A"),
            "final_intent": self.workflow_data[0]["metadata"].get("intent", "N/A"),
            "workflow_path": [item["node_name"] for item in self.workflow_data],
            "total_prompts": len(self.workflow_data),
            "total_execution_time": total_execution_time,
            "prompts": self.workflow_data
        }
        
        summary_file = self.base_path / "workflow_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

# Global instances cache to reuse PromptSaver per session
_prompt_savers = {}

def get_prompt_saver(session_id: str) -> PromptSaver:
    """Get or create PromptSaver instance for session"""
    if session_id not in _prompt_savers:
        _prompt_savers[session_id] = PromptSaver(session_id)
    return _prompt_savers[session_id]

def log_prompt_if_enabled(node_name: str, prompt: str, state: ConversationState, model_name: str = None, temperature: float = None, llm_response_time: float = None) -> None:
    """
    Universal function to log prompts if SAVE_PROMPT is enabled
    
    Args:
        node_name: Name of the workflow node
        prompt: The prompt content
        state: ConversationState with session info
        model_name: Model name used for this prompt
        temperature: Temperature used for this prompt
        llm_response_time: LLM response time in milliseconds
    """
    from config import SAVE_PROMPT
    
    if not SAVE_PROMPT:
        return
        
    session_id = state.get("session_id", "unknown")
    saver = get_prompt_saver(session_id)
    
    metadata = {
        "user_query": state.get("user_query", "N/A"),
        "intent": str(state.get("intent", "N/A")),
        "detected_language": state.get("detected_language", "N/A"),
        "model_name": model_name,
        "temperature": temperature,
        "llm_response_time": llm_response_time,
        "chat_history_length": len(state.get("chat_history", []))
    }
    
    saver.save_prompt(node_name, prompt, metadata)

def save_workflow_summary_if_enabled(state: ConversationState, total_execution_time: float = None) -> None:
    """
    Save workflow summary if SAVE_PROMPT is enabled
    
    Args:
        state: ConversationState with session info
        total_execution_time: Total workflow execution time
    """
    from config import SAVE_PROMPT
    
    if not SAVE_PROMPT:
        return
        
    session_id = state.get("session_id", "unknown")
    if session_id in _prompt_savers:
        saver = _prompt_savers[session_id]
        saver.save_workflow_summary(total_execution_time)
        
        # Cleanup after saving summary
        del _prompt_savers[session_id]